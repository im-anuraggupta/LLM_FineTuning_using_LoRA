{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn7lbG3A5ErR"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# !pip install -q -U transformers datasets accelerate peft bitsandbytes\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni37NJu45OCo",
        "outputId": "caae0564-6012-4b64-a2f3-699c05860039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 14.7 GB\n",
            "Loading tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# Set environment variable to avoid tokenizers parallelism warning\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Model name\n",
        "model_name = \"tiiuae/falcon-rw-1b\"\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    load_in_8bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INfnUJZC5Vbe",
        "outputId": "0138e3b9-433b-4261-d206-4be40cacfe9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,359,296 || all params: 1,313,984,512 || trainable%: 0.1796\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=8,  # Lower rank for Colab memory constraints\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=[\"query_key_value\", \"dense\"]\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "159c0f3ce95e4fd49057c2164bfc9049",
            "9d8f1e6e95f84dd2bf00a34a5f96edd0",
            "1a69eca805d84125a5aacaeb1cd22886",
            "df4aeca8275644cbad6b873d7ae21cf9",
            "922642cdbd194445b0181f35b092d51e",
            "72ff5dac86ee4931a92ae4aa7b7956a6",
            "d1226965aa6f41fa882ebe077589a4ae",
            "f2d28fa2391b4ca889405128f54463d2",
            "457687a078f44ffb83d0eb12a617211d",
            "12c4d0c11b674c89b887b2674bcb39d3",
            "3386e218684346a38d0064a2ac527317",
            "ced19961b014467d8be4a22a4c201777",
            "ae9f60f4d671456e8101c36ec9856488",
            "1ea7341c8c5040c6aa992b835413c180",
            "641b63cafd4a4c97a18bffe985e991a1",
            "3e4cc11f0fb74076b52251ebc9b9dc05",
            "9604b5bfbedb4ce096f9fbdf967db0b4",
            "03c520e860364380a15e59581a2cb849",
            "5e5ae6106a50462b855d24b33141a200",
            "abf3495bd0424d30a2e7a955d5a68942",
            "2cdd3e80b7044eb4883cb4afd4e07649",
            "f379416e82c349ffa1905eab4965b16d"
          ]
        },
        "id": "vE4tqC645NbV",
        "outputId": "54223e1c-4ac1-451f-c49f-bffd70f226d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 5850\n",
            "Test samples: 650\n",
            "Tokenizing dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "159c0f3ce95e4fd49057c2164bfc9049",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5850 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ced19961b014467d8be4a22a4c201777",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/650 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load and prepare dataset\n",
        "dataset = load_dataset(\"yelp_review_full\", split=\"train[:1%]\")  # Even smaller for Colab\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "print(f\"Train samples: {len(dataset['train'])}\")\n",
        "print(f\"Test samples: {len(dataset['test'])}\")\n",
        "\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    texts = [f\"Review: {text}\" for text in examples[\"text\"]]\n",
        "\n",
        "    tokens = tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,  # Keep shorter for Colab\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
        "    return tokens\n",
        "\n",
        "# Tokenize dataset\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset['train'].column_names\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lOjXC1M5o1S",
        "outputId": "5e4f42aa-93df-4554-f2dc-4beda64b8248"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2332070506.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-falcon-model\",\n",
        "    per_device_train_batch_size=4,  # Very small batch size for Colab\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=8,  # Compensate with gradient accumulation\n",
        "    num_train_epochs=1,  # Fewer epochs for faster training\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    eval_strategy=\"steps\",\n",
        "    fp16=True,  # Always use fp16 on Colab\n",
        "    dataloader_pin_memory=False,  # Disable for Colab\n",
        "    dataloader_num_workers=0,  # Disable multiprocessing\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=None,\n",
        ")\n",
        "\n",
        "# Suppress the label_names warning\n",
        "trainer.label_names = [\"labels\"]\n",
        "\n",
        "# Clear cache before training\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "R9a3V-F_4_hI",
        "outputId": "438b210c-4a3d-4909-e74f-7283a19e5670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [183/183 13:29, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.210600</td>\n",
              "      <td>2.164011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.202100</td>\n",
              "      <td>2.144062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.122900</td>\n",
              "      <td>2.138256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model...\n",
            "Training completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "\n",
        "trainer.train()\n",
        "# Save the final model\n",
        "print(\"Saving model...\")\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(\"./lora-falcon-model\")\n",
        "print(\"Training completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKLk7tJj5AIe",
        "outputId": "ba31f9dd-73d8-43b5-f781-1bfd8268215f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:93: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated: Review: This restaurant is great, and I love the decor. The food is good, but not exceptional. The best thing about it is the service. The wait staff\n"
          ]
        }
      ],
      "source": [
        "#Test the model\n",
        "model.eval()\n",
        "# Merge and unload the adapter for generation\n",
        "model = model.merge_and_unload()\n",
        "test_prompt = \"Review: This restaurant\"\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
        "# Move inputs to the same device as model\n",
        "if torch.cuda.is_available():\n",
        "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=30,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        use_cache=False,  # Disable cache to avoid the error\n",
        "    )\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Generated: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH655MLmKqx1",
        "outputId": "02edc7ea-fac6-49b6-9f6e-030402634c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated: The food was... ok, but nothing particularly memorable. I ordered the shrimp and grits, which was pretty good. The grits were huge and fluffy. The shrimp were also good, but a little dry. The sides were pretty standard, and the bread pudding was\n"
          ]
        }
      ],
      "source": [
        "test_prompt = \"The food was ...\"\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
        "# Move inputs to the same device as model\n",
        "if torch.cuda.is_available():\n",
        "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        use_cache=False,  # Disable cache to avoid the error\n",
        "    )\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Generated: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Der9lsa9E71k",
        "outputId": "16ed20ed-6de2-42b9-e98f-1f429734efdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory cleared\n"
          ]
        }
      ],
      "source": [
        "# Clear cache after training\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Memory cleared\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
